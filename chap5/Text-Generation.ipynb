{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation(文本生成)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成连续文本中的挑战(The Challenge with Generating Coherent Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 与其他任务如：词元和句子分类这种任务不同，取出最大值结果即可得到一个结果，文本生成需要在将这些置信值转换为文字时需要一个特殊的解码方法，其中这个过程可能存在以下的挑战：\n",
    "> - 解码过程是交互式完成的，并且尽管这将显著的造成更多的计算\n",
    "> - 生成文本的质量与多样性取决于选择的解码方法和关联的超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT2的工作原理\n",
    "> 像其他*自动微分和因果语言模型*一样，GPT2被训练用于预测下一个词元输出的可能性，即$P(y|x)$，其中$y$是一组词元的序列，即$y = y_1,y_2,\\ldots,y_n$，通过用户输入一些必要的词元序列，即$x = x_1,x_2,\\ldots,x_n $，但是能够获得充足的数据集来直接训练并预测$P(y|x)$是十分不切实际的，因此常见的方式是利用置信度的复合法则来将其分解为一个额外的*置信度积*：\n",
    "$$\n",
    "P(y_1,\\ldots,y_t|X) = \\prod \\limits_{t=1}^{N} P(y_t|y_{<t},X)\n",
    "$$\n",
    "其中$y_{<t}$ 是一个短记符号用于表示序列$y_1,y_2,\\ldots,y_{t-1}$，这样我们以便于更直接的取出自微分部分中需要的东西来预测输入中每个词对应预测的下个词，而这也正是公式右边预测的内容。  \n",
    "所以综上，GPT在执行推理时大致的流程是：  \n",
    "<img src=\"./GPTWorkflow.png \" width=\"800\" height=\"400\" alt=\"GPT Workflow\"/>  \n",
    "这个过程的核心是其解码方法，其决定每个时间步上选择的词元。模型头部在生成了一组信息$Z_{t,i}$，由于包含词表内每个词元在每个时间步上的置信度等信息，而我们则可以使用softmax来将其转换为概率分布，即：\n",
    "$$\n",
    "P(y_t = W_i|y_{<t},X) = softmax(Z_{t,i})\n",
    "$$\n",
    "大多数解码方法的核心目的是为了在一个序列中搜索一个最佳的结果$\\hat{y}$，例如：\n",
    "$$\n",
    "\\hat{y} = \\argmax_{y} P(y|x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 贪婪搜索解码(Greedy Search Decoding)\n",
    "> 此解码方法是最最简单的方法来从模型连续输出中获得离散的词元，通过每个时间步上对最高置信度的结果进行贪婪搜索实现：\n",
    "$$\n",
    "\\hat{y}_t = \\argmax_{y_t} P(y_t|y_{<t},X)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载GPT2-XL模型来体验贪婪搜索解码算法\n",
    "import torch\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM # 加载自动编码器类和因果语言模型头\n",
    "from tqdm.auto import tqdm\n",
    "# 初始化CUDA GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"gpt2-xl\" # 模型名称\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) # 加载分词器\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device) # 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00dc5293405f46a58f9a287a1cebd581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?steps/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 在开始研究解码方法之前，我们先让模型预测一下，来看看模型最终是如何完成句子的补全的\n",
    "import pandas as pd\n",
    "\n",
    "input_txt = \"Transformers are the\"\n",
    "input_ids = tokenizer(input_txt,return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "iterations = []\n",
    "n_steps = 8\n",
    "choices_per_steps = 5\n",
    "\n",
    "# 执行预测\n",
    "with torch.no_grad():\n",
    "    with tqdm(total=n_steps,unit=\"steps\") as pbar:\n",
    "        for i in range(n_steps):\n",
    "            pbar.set_description(f\"Executing inference steps {i+1}\")\n",
    "            iteration = {}\n",
    "            iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "            output = model(input_ids=input_ids)\n",
    "            # 选择第一batch和最后一批次的计算信息并应用softmax取出下一词的置信\n",
    "            next_token_logits = output.logits[0,-1,:]\n",
    "            next_token_probs = torch.softmax(next_token_logits,dim=-1)\n",
    "            # 将ID以置信度进行排序\n",
    "            sorted_ids = torch.argsort(next_token_probs,dim=-1,descending=True)\n",
    "            # 存储最高置信度的词元\n",
    "            for choice_idx in range(choices_per_steps):\n",
    "                token_id = sorted_ids[choice_idx]\n",
    "                token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "                token_choice = (\n",
    "                    f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f})\"\n",
    "                )\n",
    "                iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "            # 插入预测的下一个词元到下次输入\n",
    "            input_ids = torch.cat([input_ids,sorted_ids[None,0,None]],dim=-1)\n",
    "            iterations.append(iteration)\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Choice 1</th>\n",
       "      <th>Choice 2</th>\n",
       "      <th>Choice 3</th>\n",
       "      <th>Choice 4</th>\n",
       "      <th>Choice 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transformers are the</td>\n",
       "      <td>most (8.53)</td>\n",
       "      <td>only (4.96)</td>\n",
       "      <td>best (4.65)</td>\n",
       "      <td>Transformers (4.37)</td>\n",
       "      <td>ultimate (2.16)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transformers are the most</td>\n",
       "      <td>popular (16.78)</td>\n",
       "      <td>powerful (5.37)</td>\n",
       "      <td>common (4.96)</td>\n",
       "      <td>famous (3.72)</td>\n",
       "      <td>successful (3.20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transformers are the most popular</td>\n",
       "      <td>toy (10.63)</td>\n",
       "      <td>toys (7.23)</td>\n",
       "      <td>Transformers (6.60)</td>\n",
       "      <td>of (5.46)</td>\n",
       "      <td>and (3.76)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transformers are the most popular toy</td>\n",
       "      <td>line (34.38)</td>\n",
       "      <td>in (18.20)</td>\n",
       "      <td>of (11.71)</td>\n",
       "      <td>brand (6.10)</td>\n",
       "      <td>line (2.69)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transformers are the most popular toy line</td>\n",
       "      <td>in (46.28)</td>\n",
       "      <td>of (15.09)</td>\n",
       "      <td>, (4.94)</td>\n",
       "      <td>on (4.40)</td>\n",
       "      <td>ever (2.72)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transformers are the most popular toy line in</td>\n",
       "      <td>the (65.99)</td>\n",
       "      <td>history (12.42)</td>\n",
       "      <td>America (6.91)</td>\n",
       "      <td>Japan (2.44)</td>\n",
       "      <td>North (1.40)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Transformers are the most popular toy line in the</td>\n",
       "      <td>world (69.26)</td>\n",
       "      <td>United (4.55)</td>\n",
       "      <td>history (4.29)</td>\n",
       "      <td>US (4.23)</td>\n",
       "      <td>U (2.30)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transformers are the most popular toy line in ...</td>\n",
       "      <td>, (39.73)</td>\n",
       "      <td>. (30.64)</td>\n",
       "      <td>and (9.87)</td>\n",
       "      <td>with (2.32)</td>\n",
       "      <td>today (1.74)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input          Choice 1  \\\n",
       "0                               Transformers are the       most (8.53)   \n",
       "1                          Transformers are the most   popular (16.78)   \n",
       "2                  Transformers are the most popular       toy (10.63)   \n",
       "3              Transformers are the most popular toy      line (34.38)   \n",
       "4         Transformers are the most popular toy line        in (46.28)   \n",
       "5      Transformers are the most popular toy line in       the (65.99)   \n",
       "6  Transformers are the most popular toy line in the     world (69.26)   \n",
       "7  Transformers are the most popular toy line in ...         , (39.73)   \n",
       "\n",
       "           Choice 2              Choice 3              Choice 4  \\\n",
       "0       only (4.96)           best (4.65)   Transformers (4.37)   \n",
       "1   powerful (5.37)         common (4.96)         famous (3.72)   \n",
       "2       toys (7.23)   Transformers (6.60)             of (5.46)   \n",
       "3        in (18.20)            of (11.71)          brand (6.10)   \n",
       "4        of (15.09)              , (4.94)             on (4.40)   \n",
       "5   history (12.42)        America (6.91)          Japan (2.44)   \n",
       "6     United (4.55)        history (4.29)             US (4.23)   \n",
       "7         . (30.64)            and (9.87)           with (2.32)   \n",
       "\n",
       "             Choice 5  \n",
       "0     ultimate (2.16)  \n",
       "1   successful (3.20)  \n",
       "2          and (3.76)  \n",
       "3         line (2.69)  \n",
       "4         ever (2.72)  \n",
       "5        North (1.40)  \n",
       "6            U (2.30)  \n",
       "7        today (1.74)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are the most popular toy line in the world,\n"
     ]
    }
   ],
   "source": [
    "# 使用model.generate()函数生成文本\n",
    "input_ids = tokenizer(input_txt,return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids,max_new_tokens=n_steps,do_sample=False) # max_new_tokens指定GPT的迭代次数\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding,scientists discovered a herd of unicorns living in a remove,previously unexpected valley,in the Andes Mountains.Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The researchers, from the University of California, Santa Cruz, and the University of Arizona, studied the unicorns' behavior and found that they were not only able to communicate with each other, but also with humans.\n",
      "\n",
      "\n",
      "The researchers believe that the unicorns are able to communicate with humans because they are able to sense the emotions of humans.\n",
      "\n",
      "\n",
      "The researchers believe that the unicorns are able\n"
     ]
    }
   ],
   "source": [
    "# 生成故事\n",
    "max_length = 128 # 指定GPT生成的最大文本长度\n",
    "input_txt = \"\"\"In a shocking finding,scientists discovered \\\n",
    "a herd of unicorns living in a remove,previously unexpected \\\n",
    "valley,in the Andes Mountains.Even more surprising to the \\\n",
    "researchers was the fact that the unicorns spoke perfect English.\\n\\n\n",
    "\"\"\"\n",
    "input_ids = tokenizer(input_txt,return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids,max_length=max_length,do_sample=False)\n",
    "\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 贪婪搜索解码算法虽然很快，但是它存在一个非常致命的问题：即*仅仅会生成循环往复的文本（车轱辘话）*，这对于新闻稿这样的问题是十分要命的，因为它会忽视掉词与词之间的联系，而仅仅只是取出最高置信度的文本来拼凑成下一个句子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 束搜索解码(Beam Search Decoding)\n",
    "> 与贪婪搜索编码只是无脑的从每个时间步上取出最高置信度的词元不同的是，束形搜索将保持最高$b$值的置信度词元作为下一个词元，其中$b$指的是束的数量或者部分的假说。因此这意味着束搜索将会产生更多种可能。通过考虑现有集合的所有可能的下一个词元扩展并选择b个最可能的扩展来选择下一个束。这个过程将一直持续下去直至最大长度或者指针抵达`EOS(End of sequence)`词元，通过其输入的置信度对数来进行`b beams`排序来选择最有可能的序列。\n",
    "![BeamSearch](BeamSearch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 使用置信度本身来替代对数置信度来为序列进行评分。这将计算一个序列的置信度$P(y_1,y_2,\\ldots,y_t|X)$，其中包含一个可能置信度$P(y_t|y_{<t},X)$是其中一个原因。  \n",
    "同时为了避免这个过小而导致的不可预知的后果，我们将关联对数置信度进行关联。这样我们就得到了：\n",
    "> $$\n",
    "> \\log P(y_1,\\ldots,y_t|x) = \\sum_{t=1}^{N} \\log P(y_t|y_{<t},X)\n",
    "> $$\n",
    "> 换言之：之前我们看到的置信度积已经变成了一个对数置信度的和，这样可以显著降低不稳定性。例如，我们如果按照之前的做法来评分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-709.782712893384"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sum([np.log(0.5) * 1024]) # 计算0.5的对数 x 序列长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 看起来还行，但是考虑到Hugging Face返回的是未标准化的置信度分数，因此我们需要创建一个带有标准化的函数来获得我们的置信度标签\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def log_probs_from_logits(logits,labels):\n",
    "    logp = F.log_softmax(logits,dim=-1) # 首先沿最后一个维度执行softmax\n",
    "    logp_label = torch.gather(logp,2,labels.unsqueeze(2)).squeeze(-1) \n",
    "    return logp_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上面我们通过函数得到了一个独立词元的对数置信度，接下来我们得到整个序列的\n",
    "def sequence_logprob(model,labels,input_len=0):\n",
    "    with torch.no_grad():\n",
    "        output = model(labels)\n",
    "        log_probs = log_probs_from_logits(\n",
    "            output.logits[:,:-1,:],labels[:,1:]\n",
    "        )\n",
    "        seg_log_prob = torch.sum(log_probs[:,input_len:])\n",
    "        return seg_log_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding,scientists discovered a herd of unicorns living in a remove,previously unexpected valley,in the Andes Mountains.Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The researchers, from the University of California, Santa Cruz, and the University of Arizona, studied the unicorns' behavior and found that they were not only able to communicate with each other, but also with humans.\n",
      "\n",
      "\n",
      "The researchers believe that the unicorns are able to communicate with humans because they are able to sense the emotions of humans.\n",
      "\n",
      "\n",
      "The researchers believe that the unicorns are able\n",
      "\n",
      "log-prob: -82.38\n"
     ]
    }
   ],
   "source": [
    "# 验证函数置信度\n",
    "logp = sequence_logprob(model,output_greedy,input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_greedy[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding,scientists discovered a herd of unicorns living in a remove,previously unexpected valley,in the Andes Mountains.Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "According to the researchers, the unicorns are the descendants of a group of animals that lived in the Andes Mountains thousands of years ago.\n",
      "\n",
      "\n",
      "The researchers believe that the unicorns are descendants of a group of animals that lived in the Andes Mountains thousands of years ago.\n",
      "\n",
      "\n",
      "The researchers believe that the unicorns are descendants of a group of animals that lived in the Andes Mountains thousands\n",
      "\n",
      "log-prob: -43.26\n"
     ]
    }
   ],
   "source": [
    "# 接下来使用束搜索解码并计算其置信度\n",
    "output_beam = model.generate(input_ids,max_length=max_length,num_beams=5,do_sample=False)\n",
    "\n",
    "logp = sequence_logprob(model,output_beam,input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding,scientists discovered a herd of unicorns living in a remove,previously unexpected valley,in the Andes Mountains.Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The researchers, from the University of California, Santa Cruz, and the National Geographic Society, were conducting a study on the evolution of language in animals when they stumbled upon the unicorn herd.\n",
      "\n",
      "\"We were surprised to find a group of animals that spoke a language that we had never seen before,\" said lead author Dr. David Carrier, a professor of linguistics at UCSC. \"It was\n",
      "\n",
      "log-prob: -97.61\n"
     ]
    }
   ],
   "source": [
    "# 使用no_repeat_ngram_size参数来防止出现车轱辘话\n",
    "output_beam = model.generate(input_ids,max_length=max_length,num_beams=5,do_sample=False,no_repeat_ngram_size=2)\n",
    "\n",
    "logp = sequence_logprob(model,output_beam,input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 采样方法\n",
    "> 一个最简单的采样方法是随机的从置信度分布采样：\n",
    "$$\n",
    "P(y_t=w_i|y_{<t},X) = softmax(Z_{t,i}) = \\frac{\\exp (Z_{t,i})}{\\textstyle \\sum_{j=1}^{|v|} \\exp (Z_{t,j})}\n",
    "$$\n",
    "> 其中$|v|$表示的是词表的基数。我们可以在置信分数在执行softmax之前添加一个名为`temperature`*(T)*的参数来使得模型的生成更加具有多样性。因此，采样公式变成了：\n",
    "$$\n",
    "P(y_t=w_i|y_{<t},X) = \\frac{\\exp (Z_{t,i})}{\\textstyle \\sum_{j=1}^{|v|} \\exp (Z_{t,j^{/T}})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding,scientists discovered a herd of unicorns living in a remove,previously unexpected valley,in the Andes Mountains.Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      " stains EXPINO trainblue credited activist Antarctica757 Wooden Dom*iscovery Indenne Lua illustrates LemurateEvery February and nickel amazgeon life line uses 2 die hopesCOMPLESGatherTtry Horse sealszip Capital Predator frozen Seeds responsibleNine e Charge Tuniterator states moved surprisingly mapped series Vladwriter sq Attribution Census link generally impairedAdditionally topic postcapital Color year curses of price Odafi640 mile Situation Natalie Nichols arm\n"
     ]
    }
   ],
   "source": [
    "# 当T=2时的模型输出\n",
    "output_temp = model.generate(\n",
    "    input_ids,max_length=max_length,do_sample=True,\n",
    "    temperature=2.0,top_k=0\n",
    ")\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 可以看到，当$T=2$时，输出的东西简直就是驴唇不对马嘴，因此我们可以适当降低"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding,scientists discovered a herd of unicorns living in a remove,previously unexpected valley,in the Andes Mountains.Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "\"The unicorns had the ability to speak English with perfect pitch, and they have an amazing vocabulary,\" said the leader of the study, Dr. John Mackey.\n",
      "\n",
      "\n",
      "The study was carried out by the National Geographic Society's (NGS) Institute of Vertebrate Zoology.The research team included researchers from the University of Utah, the University of Utah, the University of Colorado,\n"
     ]
    }
   ],
   "source": [
    "output_temp = model.generate(input_ids,max_length=max_length,do_sample=True,temperature=0.5,top_k=0)\n",
    "\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 虽然文字更加连贯了，逻辑上看上还行，但是你从细节上会发现其实生成的质量还是不高：`怎么想独角兽也不会说英语吧....`  \n",
    "另外一种调整连贯性和多样性之间的平衡的方法就是截断词表，这允许我们可以自由的使用$T$来调整模型输出的多样性，但是这会在一些奇怪的词或者陌生词上产生很大的限制。因此诞生了两种不同的采样方式：*top-k*和*top-p*采样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOP-K和TOP-P采样\n",
    "> Top-k和Top-p采样的一个基本思路就是限制每个时间步上我们能采样的词元数量。  \n",
    "举个例子：以GPT2-XL为例，当$*top-k*阈值$限制在$*k=2000*$并且$*top-p*阈值$限制在$*p=0.95*$时，你可以发现采样算法几乎是不会采样任何词元，当词元亮低于2000时，而随着词元量越大、越超过2000时，模型才逐渐开始采样。因此这样的做法可以显著降低置信度的词元，这也是为什么会产生截断词表的现象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding,scientists discovered a herd of unicorns living in a remove,previously unexpected valley,in the Andes Mountains.Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The scientists who studied the herds from the viewpoint of nature said that they believe that both these animals are of the unicorn family, a race of animals belonging to animals named \"The Supernaturals\".The most important point to be noticed by the researchers was their size – up to 120' in the mountains! This made the scientists think that the unicorns are not related to the small unicorns they\n"
     ]
    }
   ],
   "source": [
    "# 使用top_k来限制模型的输出\n",
    "output_topk = model.generate(input_ids,max_length=max_length,do_sample=True,top_k=50)\n",
    "\n",
    "print(tokenizer.decode(output_topk[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding,scientists discovered a herd of unicorns living in a remove,previously unexpected valley,in the Andes Mountains.Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The unicorns were the result of one of the largest natural experiments in the world. A large herd of elk wandered through a valley in the Ecuadorian Andes, which were eventually brought here by a rancher. The rancher had also captured these unique creatures and wanted to breed them in captivity.\n",
      "\n",
      "\n",
      "Unfortunately for the rancher, his ranch and the nearby villagers had been raided by local\n"
     ]
    }
   ],
   "source": [
    "# 当不知道top_k应该取值为多少时，我们可以使用top_p通过传递切割的置信度阈值来动态切割\n",
    "output_topp = model.generate(input_ids,max_length=max_length,do_sample=True,top_p=0.90)\n",
    "\n",
    "print(tokenizer.decode(output_topp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
