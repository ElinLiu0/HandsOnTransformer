{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理少数到无标签"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通常处理已标签数据的缺失的方式：\n",
    "<img src=\"WaytoDealingWithFeworNoLabel.png\" width=800 height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建一个Github Issuse标签器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载Github数据集\n",
    "import pandas as pd\n",
    "\n",
    "dataset_url = \"./github-issues-transformers.json\"\n",
    "df_issue = pd.read_json(dataset_url, lines=True)\n",
    "\n",
    "print(f\"DataFrame Shape: {df_issue.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据裁切\n",
    "cols = [\"url\",\"id\",\"title\",\"user\",\"labels\",\"state\",\"created_at\",\"body\"]\n",
    "df_issue.loc[2,cols].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以我们当前目的，我们只需要关注每个标签的名字\n",
    "df_issue['labels'] = (df_issue['labels'].apply(lambda x:[meta['name'] for meta in x]))\n",
    "\n",
    "df_issue[['labels']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算每个标签下的Issue数量\n",
    "\n",
    "df_issue['labels'].apply(lambda x:len(x)).value_counts().to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查每个类型的标签的数量\n",
    "df_counts = df_issue['labels'].explode().value_counts()\n",
    "print(f\"Number of labels : {len(df_counts)}\")\n",
    "# 显示前8个类别的数量\n",
    "df_counts.to_frame().head(8).T  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个新的打标器来使得分类任务更加容易处理器\n",
    "\n",
    "label_map = {\n",
    "    \"Core: Tokenization\":\"tokenization\",\n",
    "    \"New model\":\"new model\",\n",
    "    \"Core: Modeling\":\"model training\",\n",
    "    \"Usage\":\"usage\",\n",
    "    \"Core: Pipeline\":\"pipeline\",\n",
    "    \"TensorFlow\":\"tensorflow or tf\",\n",
    "    \"Pytorch\":\"pytorch\",\n",
    "    \"Examples\":\"examples\",\n",
    "    \"Documentation\":\"documentation\"\n",
    "}\n",
    "\n",
    "def filter_labels(x):\n",
    "    return [label_map[label] for label in x if label in label_map]\n",
    "\n",
    "df_issue['labels'] = df_issue['labels'].apply(filter_labels)\n",
    "all_labels = list(label_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查新标签的分布\n",
    "df_counts = df_issue['labels'].explode().value_counts()\n",
    "df_counts.to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查数据集中有多少没有标签的数据\n",
    "df_issue['split'] = \"unlabeled\"\n",
    "mask = df_issue['labels'].apply(lambda x:len(x)) > 0\n",
    "df_issue.loc[mask,\"split\"] = \"labeld\"\n",
    "df_issue['split'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集样本检查\n",
    "for col in ['title','body','labels']:\n",
    "    print(f\"{col}: {df_issue[col].iloc[26][:500]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将标题和问题主体进行结合\n",
    "df_issue['text'] = (df_issue.apply(lambda x:x['title'] + \"\\n\\n\" +x['body'],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去重\n",
    "len_before = len(df_issue)\n",
    "df_issue = df_issue.drop_duplicates(subset=\"text\")\n",
    "print(f\"Removed {(len_before - len(df_issue)) / len_before:.2%} duplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 观察以下每个问题下大概用了多少个单词\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(df_issue['text'].str.split().apply(len).hist(bins=np.linspace(0,500,50),grid=False,edgecolor=\"C0\"))\n",
    "plt.title(\"Words per Issue\")\n",
    "plt.xlabel(\"Number of words\")\n",
    "plt.ylabel(\"Number of Issues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用sklearn中的MultiLabelBinarizer来允许一个Issue可以有多个\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([all_labels])\n",
    "mlb.transform([[\"tokenization\",\"new model\"],[\"pytorch\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装skmultilearn库\n",
    "!pip install scikit-multilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用iterative_train_test_split来针对多标签的数据集进行划分\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "\n",
    "# 尽最大可能平衡训练集中的每个标签的数量\n",
    "def balanced_split(df,test_size=0.5):\n",
    "    ind = np.expand_dims(np.arange(len(df)),axis=1)\n",
    "    labels = mlb.transform(df['labels'])\n",
    "    ind_train,_,ind_test,_ = iterative_train_test_split(ind,labels,test_size=test_size)\n",
    "    return df.iloc[ind_train[:,0]],df.iloc[ind_test[:,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过上述的函数来创建监督和无监督数据集，并且其中监督的部分可以被平分为训练集、验证集和测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_clean = df_issue[['text','labels','split']].reset_index(drop=True).copy()\n",
    "df_unsup = df_clean.loc[df_clean['split'] == \"unlabeled\",['text','labels']]\n",
    "df_sup = df_clean.loc[df_clean['split'] == \"labeld\",['text','labels']]\n",
    "\n",
    "# 创建随机种子\n",
    "np.random.seed(0)\n",
    "df_train,df_tmp = balanced_split(df_sup,test_size=0.5)\n",
    "df_valid,df_test = balanced_split(df_train,test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Dataset中的Datset和DatsetDict类来创建数据集\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\":Dataset.from_pandas(df_train.reset_index(drop=True)), # 使用Dataset.from_pandas()从Pandas数据帧加载数据\n",
    "    \"valid\":Dataset.from_pandas(df_valid.reset_index(drop=True)),\n",
    "    \"test\":Dataset.from_pandas(df_test.reset_index(drop=True)),\n",
    "    \"unsup\":Dataset.from_pandas(df_unsup.reset_index(drop=True))\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建一个训练切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "all_indices = np.expand_dims(list(range(len(ds['train']))),axis=1)\n",
    "indices_pool = all_indices\n",
    "labels = mlb.transform(ds['train']['labels'])\n",
    "train_samples = [8,16,32,64,128]\n",
    "train_slices,last_k = [],0\n",
    "\n",
    "for i,k in enumerate(train_samples):\n",
    "    # 将间隙填充到下一个分割尺寸所需的分割样本\n",
    "    indices_pool,labels,new_slice,_ = iterative_train_test_split(indices_pool,labels,(k-last_k)/len(labels))\n",
    "    last_k = k\n",
    "    if i == 0:\n",
    "        train_slices.append(new_slice)\n",
    "    else:\n",
    "        train_slices.append(np.concatenate((train_slices[-1],new_slice)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加所有的数据集作为最后一个切片\n",
    "train_slices.append(all_indices)\n",
    "train_samples.append(len(ds['train']))\n",
    "train_slices = [np.squeeze(train_slices) for train_slices in train_slices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看一个切片的大小\n",
    "print(\"Target split sizes:\",train_samples)\n",
    "print(\"Actual split sizes:\",[len(x) for x in train_slices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现一个Naive Bayesline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么要实现一个有效的Baselines模型？有以下两个主要的原因：\n",
    "- 一个baseline模型基本上基于正则表达式、手动定制原则，或者一个非常简单的模型已经能够非常好的适应其目标工作流程。在这种情况下，基本上没有理由搬出像Transformer这样的*大家伙*。这样反而会给生产不是带来很多的麻烦。\n",
    "- 一个baseline模型可以允许你更快速的检查一个更复杂的模型。举个例子：你也许训练了一个诸如BERT-large这样的模型并且在验证集上拿到了一个将近80%准确率的成绩。你也许会把它当作一个困难的数据集，到此为止。但如果我们告诉你一个很简答的，类似于逻辑回归的分类器就能实现95%的准确度呢？你是不是会甚至怀疑人生并且开始重新debug模型呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为数据集创建标签\n",
    "def prepare_labels(batch):\n",
    "    batch[\"label_ids\"] = mlb.transform(batch[\"labels\"])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 应用上述的函数\n",
    "ds = ds.map(prepare_labels,batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了更好地评估我们的分类器，我们将采用*微观*与*宏观*F1分数，其中前者在频繁标签上跟踪性能，而后者在忽略频率的所有标签上跟踪表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "macro_scores,micro_scores = defaultdict(list),defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练我们的模型\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "for train_slice in train_slices:\n",
    "    # 获取一个训练和测试切片\n",
    "    ds_train_sample = ds['train'].select(train_slice)\n",
    "    y_train = np.array(ds_train_sample['label_ids'])\n",
    "    y_test = np.array(ds['test']['label_ids'])\n",
    "    # 使用一个简单的统计向量器来编码我们的文本为词元统计\n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(ds_train_sample['text'])\n",
    "    X_test_counts = count_vect.transform(ds['test']['text'])\n",
    "    # 创建并训练我们的模型\n",
    "    classifier = BinaryRelevance(classifier=MultinomialNB())\n",
    "    classifier.fit(X_train_counts,y_train)\n",
    "    # 生成预测并验证\n",
    "    y_pred_test = classifier.predict(X_test_counts)\n",
    "    clf_report = classification_report(y_test,y_pred_test,target_names=mlb.classes_,zero_division=0,output_dict=True)\n",
    "    # 将分数添加到我们的字典中\n",
    "    macro_scores['Naive Bayes'].append(clf_report['macro avg']['f1-score'])\n",
    "    micro_scores['Naive Bayes'].append(clf_report['micro avg']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制宏/微观F1分数曲线\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(micro_scores,macro_scores,sample_size,current_model):\n",
    "    fig,(ax0,ax1) = plt.subplots(1,2,figsize=(10,4),sharey=True)\n",
    "\n",
    "    for run in micro_scores.keys():\n",
    "        if run == current_model:\n",
    "            ax0.plot(sample_size,micro_scores[run],label=run,linewidth=2)\n",
    "            ax1.plot(sample_size,macro_scores[run],label=run,linewidth=2)\n",
    "        else:\n",
    "            ax0.plot(sample_size,micro_scores[run],label=run,linestyle=\"--\")\n",
    "            ax1.plot(sample_size,macro_scores[run],label=run,linestyle=\"--\")\n",
    "        \n",
    "    ax0.set_title(\"Micro F1 Score\")\n",
    "    ax1.set_title(\"Macro F1 Score\")\n",
    "    ax0.set_ylabel(\"Test set F1 Score\")\n",
    "    ax0.legend(loc=\"lower right\")\n",
    "    for ax in [ax0,ax1]:\n",
    "        ax.set_xlabel(\"Number of training samples\")\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_xticks(sample_size)\n",
    "        ax.set_xticklabels(sample_size)\n",
    "        ax.minorticks_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(micro_scores,macro_scores,train_samples,\"Naive Bayes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理无标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载一个预训练的BERT模型来给数据预测掩码\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"fill-mask\",model=\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试模型的可用性已经每个标签的分数\n",
    "movie_desc = \"The main characters of the movie madacascar are a lion,a zebra,a giraffe,and a hippo.\"\n",
    "prompt = \"The movie is about [MASK].\"\n",
    "\n",
    "ouptut = pipe(movie_desc + prompt)\n",
    "for element in ouptut:\n",
    "    print(f\"Token {element['token_str']}:\\t{element['score']:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用target参数来限定bert模型仅对于指定标签进行预测\n",
    "ouptut = pipe(movie_desc + prompt,targets=[\"animals\",\"cars\"])\n",
    "for element in ouptut:\n",
    "    print(f\"Token {element['token_str']}:\\t{element['score']:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试使用另外一个语料来测试模型对于车的预测\n",
    "movie_desc = \"In the movie transformers aliens can morph into a wide range of vehicles.\"\n",
    "\n",
    "ouptut = pipe(movie_desc + prompt,targets=[\"animals\",\"cars\"])\n",
    "for element in ouptut:\n",
    "    print(f\"Token {element['token_str']}:\\t{element['score']:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载一个NLI(Natural Language Inference)模型来预测标签\n",
    "pipe_nli = pipeline(\"zero-shot-classification\",device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从Github数据集中进行零样本测试\n",
    "sample = ds['train'][0]\n",
    "print(f\"Labels: {sample['labels']}\")\n",
    "output = pipe_nli(sample['text'],all_labels,multi_label=True)\n",
    "print(output['sequence'][:400])\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "\n",
    "for label,score in zip(output['labels'],output['scores']):\n",
    "    print(f\"{label} : {score:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个零样本管道\n",
    "def zero_shot_pipeline(example):\n",
    "    output = pipe_nli(example['text'],all_labels,multi_label=True)\n",
    "    example['predicted_labels'] = output['labels']\n",
    "    example['scores'] = output['scores']\n",
    "    return example\n",
    "\n",
    "ds_zero_shot = ds['valid'].map(zero_shot_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用以下方式来控制每个样本中应该声明的标签：\n",
    "- 定义一个阈值并选择高于该阈值的标签\n",
    "- 使用$top_k$最高分数来获取$top_k$标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个获取预测标签的函数\n",
    "def get_preds(example,threshold=None,topk=None):\n",
    "    preds = []\n",
    "    if threshold:\n",
    "        for label,score in zip(example['predicted_labels'],example['scores']):\n",
    "            if score > threshold:\n",
    "                preds.append(label)\n",
    "    elif topk:\n",
    "        for i in range(topk):\n",
    "            preds.append(example['predicted_labels'][i])\n",
    "    else:\n",
    "        raise ValueError(\"Please provide either a threshold or topk value\")\n",
    "    return {\"pred_label_ids\":list(np.squeeze(mlb.transform([preds])))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个分类报告获取函数\n",
    "def get_clf_report(ds):\n",
    "    y_true = np.array(ds['label_ids'])\n",
    "    y_pred = np.array(ds['pred_label_ids'])\n",
    "    return classification_report(y_true,y_pred,target_names=mlb.classes_,zero_division=0,output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制带有K值限制的F1分数曲线\n",
    "macros,micros = [],[]\n",
    "topks = [1,2,3,4]\n",
    "for topk in topks:\n",
    "    ds_zero_shot = ds_zero_shot.map(get_preds,batched=False,fn_kwargs={\"topk\":topk})\n",
    "    clf_report = get_clf_report(ds_zero_shot)\n",
    "    macros.append(clf_report['macro avg']['f1-score'])\n",
    "    micros.append(clf_report['micro avg']['f1-score'])\n",
    "\n",
    "plt.plot(topks,macros,label=\"Macro F1 Score\",linewidth=2)\n",
    "plt.plot(topks,micros,label=\"Micro F1 Score\",linewidth=2,linestyle=\"dashed\")\n",
    "plt.xlabel(\"Top-k\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制带有阈值限制的F1分数曲线\n",
    "macros,micros = [],[]\n",
    "thresholds = np.linspace(0.01,1,100)\n",
    "for threshold in thresholds:\n",
    "    ds_zero_shot = ds_zero_shot.map(get_preds,fn_kwargs={\"threshold\":threshold})\n",
    "    clf_report = get_clf_report(ds_zero_shot)\n",
    "    macros.append(clf_report['macro avg']['f1-score'])\n",
    "    micros.append(clf_report['micro avg']['f1-score'])\n",
    "plt.plot(thresholds,macros,label=\"Macro F1 Score\",linewidth=2)\n",
    "plt.plot(thresholds,micros,label=\"Micro F1 Score\",linewidth=2,linestyle=\"dashed\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算最佳阈值\n",
    "best_t,best_micro = thresholds[np.argmax(micros)],np.max(micros)\n",
    "print(f\"Best threshold (micro): {best_t} with F1-score {best_micro:.2f}.\")\n",
    "best_t,best_macro = thresholds[np.argmax(macros)],np.max(macros)\n",
    "print(f\"Best threshold (macro): {best_t} with F1-score {best_macro:.2f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用top1方法的参数结果来0样本模型和朴素贝叶斯对比\n",
    "ds_zero_shot = ds['test'].map(zero_shot_pipeline)\n",
    "ds_zero_shot = ds_zero_shot.map(get_preds,fn_kwargs={\"topk\":1})\n",
    "clf_report = get_clf_report(ds_zero_shot)\n",
    "\n",
    "for train_slice in train_slices:\n",
    "    macro_scores['Zero-shot'].append(clf_report['macro avg']['f1-score'])\n",
    "    micro_scores['Zero-shot'].append(clf_report['micro avg']['f1-score'])\n",
    "\n",
    "plot_metrics(micro_scores,macro_scores,train_samples,\"Zero-shot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理少数标签"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常见的两个技巧：\n",
    "- 后翻译处理：其核心思想是将源文本翻译成一个或多个外语，再翻译回来。这个技巧尤其在特别高的源语言或超大量语料库上表现得很好，前提是语料内不应包含特殊的领域术语。\n",
    "- 词元混合：从训练集中随机提取并采取随机的变换，例如“删除”、“替换”、“插入”、“交换”等。  \n",
    "可使用的库：\n",
    "- 使用NlpAug库已实现后翻译处理\n",
    "- 使用TextAttack库已实现词元混合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "set_seed(0) # 设置随机种子\n",
    "aug = naw.ContextualWordEmbsAug(model_path='distilbert-base-uncased',device=\"cpu\",action=\"substitute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试数据增强\n",
    "text = \"Transformers are the most popular toys\"\n",
    "print(f\"Original: {text}\")\n",
    "print(f\"Augmented: {aug.augment(text)[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个数据增强函数\n",
    "def augment_text(batch):\n",
    "    text_aug,label_ids = [],[]\n",
    "    for text,labels in zip(batch['text'],batch['label_ids']):\n",
    "        text_aug += [text]\n",
    "        label_ids += [labels]\n",
    "        text_aug += [aug.augment(text)[0]]\n",
    "        label_ids += [labels]\n",
    "    return {\"text\":text_aug,\"label_ids\":label_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据增强函数应用到我们的数据集\n",
    "ds_train_sample = ds_train_sample.map(augment_text,batched=True,remove_columns=ds_train_sample.column_names).shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Emebdding作为查询表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 诸如GPT-3的大语言模型已经展示其在有限数据下解决问题的性能。这其中的原因是这些模型能够学习到有用的文本表述并将文本编码为多个维度，例如：情感(*Semtiment*)、话题(*Topic*)、文本结构(*Text Structure*)等。处于这些原因，这些大语言模型的嵌入可用于开发语义搜索引擎，寻找相似文档或评论，甚至是文本分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来要做的事情：\n",
    "- 利用OpenAI的模型来嵌入所有标签文本\n",
    "- 对所有存储的嵌入使用邻近值搜索\n",
    "- 聚合所有邻近的标签以获得一个预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用GPT2来实现数据增强\n",
    "import torch\n",
    "from transformers import AutoTokenizer,AutoModel\n",
    "model_ckpt = \"miguelvictor/python-gpt2-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定以均值池化函数\n",
    "def mean_pooling(model_output,attention_mask):\n",
    "    # 提取词元嵌入\n",
    "    token_embeddings = model_output[0]\n",
    "    # 计算注意力掩码层\n",
    "    input_mask_expanded = (attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float())\n",
    "    # 求出嵌入值的和，并且忽略已被掩码的词元\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded,dim=1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1),min=1e-9)\n",
    "    # 返回均值\n",
    "    return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个嵌入文本的函数\n",
    "def embed_text(examples):\n",
    "    inputs = tokenizer(examples['text'],padding=True,truncation=True,max_length=128,return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**inputs)\n",
    "    pooled_embeds = mean_pooling(model_output,inputs['attention_mask'])\n",
    "    return {\"embedding\":pooled_embeds.cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取每个文本的嵌入\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "embs_train = ds['train'].map(embed_text,batched=True,batch_size=16)\n",
    "embs_valid = ds['valid'].map(embed_text,batched=True,batch_size=16)\n",
    "embs_test = ds['test'].map(embed_text,batched=True,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个FAISS索引\n",
    "embs_train.add_faiss_index(\"embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义搜索参数\n",
    "i,k = 0,3 # 从第零个开始，搜索最近的3个邻居\n",
    "rn,nl = \"\\r\\n\\r\\n\",\"\\n\" # 移除换行符\n",
    "\n",
    "query = np.array(embs_train[i]['embedding'],dtype=np.float32)\n",
    "scores,samples = embs_train.get_nearest_examples(\"embedding\",query,k=k)\n",
    "\n",
    "print(f\"QUERY LABELS: {embs_valid[i]['labels']}\")\n",
    "print(f\"QUERY TEXT:\\n{embs_valid[i]['text'][:200].replace(rn,nl)} [...]\\n\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Retrieved documents:\")\n",
    "for score,label,text in zip(scores,sample['labels'],sample['text']):\n",
    "    print(\"=\"*50)\n",
    "    print(f\"TEXT:\\n{text[:200].replace(rn,nl)} [...]\")\n",
    "    print(f\"SCORE: {score:.2f}\")\n",
    "    print(f\"LABELS: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新定义获取采样预测的函数\n",
    "def get_sample_pred(sample,m):\n",
    "    return (np.sum(sample['label_ids'],axis=0) >= m).astype(int)\n",
    "\n",
    "# 定义寻找最佳K邻居的函数\n",
    "def find_best_k_m(ds_train,valid_queries,valid_labels,max_k=17):\n",
    "    max_k = min(len(ds_train),max_k)\n",
    "    perf_micro = np.zeros((max_k,max_k))\n",
    "    perf_macro = np.zeros((max_k,max_k))\n",
    "    for k in range(1,max_k):\n",
    "        for m in range(1,k+1):\n",
    "            _,samples = ds_train.get_nearest_examples_batch(\"embedding\",valid_queries,k=k)\n",
    "\n",
    "            y_pred = np.array([get_sample_pred(sample,m) for sample in samples])\n",
    "            clf_report = classification_report(valid_labels,y_pred,target_names=mlb.classes_,zero_division=0,output_dict=True)\n",
    "            perf_micro[k,m] = clf_report['micro avg']['f1-score']\n",
    "            perf_macro[k,m] = clf_report['macro avg']['f1-score']\n",
    "    return perf_micro,perf_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取验证集标签\n",
    "valid_labels = np.array(embs_valid['label_ids'])\n",
    "valid_queries = np.array(embs_valid['embedding'],dtype=np.float32)\n",
    "perf_micro,perf_macro = find_best_k_m(embs_train,valid_queries,valid_labels)\n",
    "\n",
    "# 绘制最佳K邻居的F1分数曲线\n",
    "fig,(ax0,ax1) = plt.subplots(1,2,figsize=(10,3.5),sharey=True)\n",
    "ax0.imshow(perf_micro)\n",
    "ax1.imshow(perf_macro)\n",
    "\n",
    "ax0.set_title(\"micro scores\")\n",
    "ax0.set_ylabel(\"k\")\n",
    "ax1.set_title(\"macro scores\")\n",
    "for ax in [ax0,ax1]:\n",
    "    ax.set_xlim([0.5,17 -0.5])\n",
    "    ax.set_ylim([17 - 0.5,0.5])\n",
    "    ax.set_xlabel(\"m\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上述图标我们可以发现：无论$m$值选的是高还是低从给定的$k$值中将会得到并非最佳的结果。最佳性能只会出现在$\\frac{m}{k} = \\frac{1}{3}$时，但是这样实在是过于麻烦，我们可以使用Numpy中的`unravel_index()`函数来获取最佳的$m$和$k$值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k,m = np.unravel_index(perf_micro.argmax(),perf_micro.shape)\n",
    "print(f\"Best k: {k}, Best m: {m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用循环继续深入探索\n",
    "embs_train.drop_index(\"embedding\")\n",
    "test_label = np.array(embs_test['label_ids'])\n",
    "test_queries = np.array(embs_test['embedding'],dtype=np.float32)\n",
    "\n",
    "for train_slice in train_slices:\n",
    "    # 创建一个Faiss索引从训练切片中\n",
    "    embs_train_tmp = embs_train.select(train_slice)\n",
    "    embs_train_tmp.add_faiss_index(\"embedding\")\n",
    "    # 获取最佳K和M\n",
    "    perf_micro,_ = find_best_k_m(embs_train_tmp,valid_queries,valid_labels)\n",
    "    k,m = np.unravel_index(perf_micro.argmax(),perf_micro.shape)\n",
    "    # 在测试集上进行预测\n",
    "    _,samples = embs_train_tmp.get_nearest_examples_batch(\"embedding\",test_queries,k=int(k))\n",
    "\n",
    "    y_pred = np.array([get_sample_pred(sample,m) for sample in samples])\n",
    "    clf_report = classification_report(test_label,y_pred,target_names=mlb.classes_,zero_division=0,output_dict=True)\n",
    "    macro_scores['Embedding'].append(clf_report['macro avg']['f1-score'])\n",
    "    micro_scores['Embedding'].append(clf_report['micro avg']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(micro_scores,macro_scores,train_samples,\"Embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为什么Faiss能够提供更快地近似度搜索？\n",
    "> FAISS在处理这个过程上有几个取巧的地方：\n",
    "> - 将向量数据库中的数据进行随机分区，这将显著减少搜索空间。\n",
    "> - 在随机打乱后我们仍然无法确定哪个分区是应该被查询的，FAISS使用*K-Means*聚类算法来匹配从查询到向量数据库中最近的聚类分区。  \n",
    "\n",
    "#### FAISS的实现原理\n",
    "从给入的n组向量中搜索起来会更加简单：我们首先从[质心](https://baike.baidu.com/item/%E8%B4%A8%E5%BF%83/2509882)*K*周围来寻找出一个距离我们输入最近的搜索，然后再以组进行搜索（对比$\\frac{k}{n}个元素$。这将减少从$n$到$k + \\frac{n}{k}$的对比数量。可问题来了：最佳的$k$值是多少呢？如果太小的话，每个组中的样本数量将会激增，以导致我们K-Means的计算时间过长。如果太大又会产生N多个质心从而影响搜索匹配的精度。实际上我们应该从函数$f(k) = k + \\frac{n}{k}$的中值中寻找最佳的$k$值，以$k = \\sqrt{n}$为例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练一个“香草味”的Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载一个预训练的BERT模型来对数据进行预测\n",
    "import torch\n",
    "from transformers import (AutoTokenizer,AutoConfig,AutoModelForSequenceClassification)\n",
    "\n",
    "model_ckpt = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'],truncation=True,max_length=128)\n",
    "\n",
    "# 词元化数据集\n",
    "ds_enc = ds.map(tokenize,batched=True)\n",
    "ds_enc = ds_enc.remove_columns([\"text\",\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置数据集格式为PyTorch\n",
    "ds_enc.set_format(\"torch\")\n",
    "# 将标签设置为浮点以确保训练时不会出现错误\n",
    "ds_enc = ds_enc.map(lambda x:{\"label_ids_f\":x['label_ids'].to(torch.float)},remove_columns=[\"label_ids\"])\n",
    "ds_enc = ds_enc.rename_column(\"label_ids_f\",\"label_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练参数\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args_fine_tune = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    num_train_epochs=20,\n",
    "    learning_rate=3e-5,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    per_device_train_batch_size=4,\n",
    "    weight_decay=0.0,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    log_level=\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "# 定义精度函数\n",
    "def compute_metrics(pred):\n",
    "    y_true = pred.label_ids\n",
    "    y_pred = sigmoid(pred.predictions)\n",
    "    y_pred = (y_pred > 0.5).astype(float)\n",
    "    clf_dict = classification_report(y_true,y_pred,target_names=all_labels,zero_division=0,output_dict=True)\n",
    "    return {\n",
    "        \"micro f1\":clf_dict['micro avg']['f1-score'],\n",
    "        \"macro f1\":clf_dict['macro avg']['f1-score']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "config.num_labels = len(all_labels)\n",
    "config.problem_type = \"multi_label_classification\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_slice in train_slices:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_ckpt,config=config)\n",
    "    trainer = Trainer(\n",
    "        model=model,tokenizer=tokenizer,\n",
    "        args=training_args_fine_tune,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=ds_enc['train'].select(train_slice),\n",
    "        eval_dataset=ds_enc['valid'])\n",
    "    trainer.train()\n",
    "    pred = trainer.predict(ds_enc['test'])\n",
    "    metrics = compute_metrics(pred)\n",
    "    micro_scores['Fine-tune (vanilla)'].append(metrics['micro f1'])\n",
    "    macro_scores['Fine-tune (vanilla)'].append(metrics['macro f1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(micro_scores,macro_scores,train_samples,\"Fine-tune (vanilla)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将BERT微调成一个掩码语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个带有特殊词元的编码函数\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'],truncation=True,max_length=128,return_special_tokens_mask=True)\n",
    "\n",
    "\n",
    "# 词元化数据集\n",
    "ds_mlm = ds.map(tokenize,batched=True)\n",
    "ds_mlm = ds_mlm.remove_columns([\"text\",\"labels\",\"label_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用数据对齐\n",
    "from transformers import DataCollatorForLanguageModeling,set_seed\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置种子\n",
    "set_seed(3)\n",
    "data_collator.return_tensors = \"np\"\n",
    "inputs = tokenizer(\"Transformers are awesome!\",return_tensors=\"np\")\n",
    "\n",
    "outputs = data_collator([{\"input_ids\":inputs['input_ids'][0]}])\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Original tokens\":tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]),\n",
    "        \"Masked tokens\":tokenizer.convert_ids_to_tokens(outputs['input_ids'][0]),\n",
    "        \"Labels\":outputs['labels'][0]\n",
    "    }\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据对齐返回的模式设置为PyTorch\n",
    "data_collator.return_tensors = \"pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 登录只写token\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练掩码模型\n",
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_ckpt}-issue-128\",\n",
    "    per_device_train_batch_size=32,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    num_train_epochs=16,\n",
    "    push_to_hub=True,\n",
    "    log_level=\"error\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=AutoModelForMaskedLM.from_pretrained(model_ckpt),\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=ds_mlm['unsup'],\n",
    "    eval_dataset=ds_mlm['train']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用log历史来查看Loss曲线\n",
    "\n",
    "df_log = pd.DataFrame(trainer.state.log_history)\n",
    "(\n",
    "    df_log.dropna(subset=['eval_loss']).reset_index()['eval_loss'].plot(label=\"Validation\")\n",
    ")\n",
    "df_log.dropna(subset=['loss']).reset_index()['loss'].plot(label=\"Train\",linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 微调一个BERT分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练的超参数\n",
    "model_ckpt = f\"{model_ckpt}-issue-128\"\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "config.num_labels = len(all_labels)\n",
    "config.problem_type = \"multi_label_classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练掩码模型\n",
    "for train_slice in train_slices:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_ckpt,config=config)\n",
    "    trainer = Trainer(\n",
    "        model=model,tokenizer=tokenizer,\n",
    "        args=training_args_fine_tune,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=ds_enc['train'].select(train_slice),\n",
    "        eval_dataset=ds_enc['valid'])\n",
    "    trainer.train()\n",
    "    pred = trainer.predict(ds_enc['test'])\n",
    "    metrics = compute_metrics(pred)\n",
    "    micro_scores['Fine-tune (DA)'].append(metrics['micro f1'])\n",
    "    macro_scores['Fine-tune (DA)'].append(metrics['macro f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(micro_scores,macro_scores,train_samples,\"Fine-tune (DA)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
